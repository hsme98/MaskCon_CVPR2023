{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1016560f-8898-489e-bd5e-5f1839a8732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import *\n",
    "from models import *\n",
    "from utils import *\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c7d7d9-94c3-4e78-8480-2a157a9b0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet\n",
    "\n",
    "dim_dict = {\n",
    "    'resnet18': 512,\n",
    "    'resnet34': 512,\n",
    "    'resnet50': 2048,\n",
    "}\n",
    "\n",
    "\n",
    "class ModelBase(nn.Module):\n",
    "    \"\"\"\n",
    "    For small size figures:\n",
    "    (i) replaces conv1 with kernel=3, str=1\n",
    "    (ii) removes pool1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, figsize=32, num_classes=10, projection_dim=128, arch=None):\n",
    "        super(ModelBase, self).__init__()\n",
    "        resnet_arch = getattr(resnet, arch)\n",
    "\n",
    "        self.net = resnet_arch(pretrained=True)\n",
    "        if figsize <= 64:  # adapt to small-size images\n",
    "            self.net.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.net.maxpool = nn.Identity()\n",
    "        self.net.fc = nn.Identity()\n",
    "\n",
    "        self.feat_dim = dim_dict[arch]\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(self.feat_dim, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, projection_dim)\n",
    "        )\n",
    "\n",
    "        self.classifer = nn.Linear(self.feat_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, feat=False):\n",
    "        x = self.net(x)\n",
    "        if feat:\n",
    "            return x\n",
    "        else:\n",
    "            cls, proj = self.classifer(x), self.projector(x)\n",
    "            return cls, proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d273b81c-4a77-43e7-96f0-8f3c2c327a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConNew(nn.Module):\n",
    "    def __init__(self, num_classes_coarse=10,  dim=128, K=4096, m=0.9, T1=0.1, T2=0.1, arch='resnet18', mode='mixcon', size=32):\n",
    "        '''\n",
    "        Modifed based on MoCo framework.\n",
    "\n",
    "        :param num_classes_coarse: num of coarse classes\n",
    "        :param dim: dimension of feature projections\n",
    "        :param K: size of memory bank\n",
    "        :param m: momentum encoder\n",
    "        :param T1: temperature of original contrastive loss\n",
    "        :param T2: temperature for soft labels generation\n",
    "        :param arch: architecture of encoder\n",
    "        :param mode: method mode [maskcon, grafit or coins]\n",
    "        :param size: dataset image size\n",
    "        '''\n",
    "        super(MaskConNew, self).__init__()\n",
    "        self.K = K\n",
    "        self.m = m\n",
    "        self.T1 = T1\n",
    "        self.T2 = T2\n",
    "        self.mode = mode\n",
    "        # create the encoders\n",
    "        self.encoder_q = ModelBase(figsize=size, num_classes=num_classes_coarse, projection_dim=dim, arch=arch)\n",
    "        self.encoder_k = ModelBase(figsize=size, num_classes=num_classes_coarse, projection_dim=dim, arch=arch)\n",
    "\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data.copy_(param_q.data)  # initialize\n",
    "            param_k.requires_grad = False  # not update by gradient\n",
    "\n",
    "        self.num_classes_coarse = num_classes_coarse\n",
    "        # create the queue\n",
    "        self.register_buffer(\"queue\", torch.randn(dim, K))\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=0)\n",
    "        self.register_buffer('coarse_labels', torch.randint(0, num_classes_coarse, [self.K]).long())\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_key_encoder(self):\n",
    "        \"\"\"\n",
    "        Momentum update of the key encoder\n",
    "        \"\"\"\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys, coarse_labels):\n",
    "        batch_size = keys.shape[0]\n",
    "\n",
    "        ptr = int(self.queue_ptr)\n",
    "        assert self.K % batch_size == 0  # for simplicity\n",
    "\n",
    "        # replace the keys at ptr (dequeue and enqueue)\n",
    "        self.queue[:, ptr:ptr + batch_size] = keys.t()  # transpose\n",
    "        self.coarse_labels[ptr:ptr + batch_size] = coarse_labels\n",
    "\n",
    "        ptr = (ptr + batch_size) % self.K  # move pointer\n",
    "\n",
    "        self.queue_ptr[0] = ptr\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _batch_shuffle_single_gpu(self, x):\n",
    "        \"\"\"\n",
    "        Batch shuffle, for making use of BatchNorm.\n",
    "        \"\"\"\n",
    "        # random shuffle index\n",
    "        idx_shuffle = torch.randperm(x.shape[0]).cuda()\n",
    "\n",
    "        # index for restoring\n",
    "        idx_unshuffle = torch.argsort(idx_shuffle)\n",
    "\n",
    "        return x[idx_shuffle], idx_unshuffle\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _batch_unshuffle_single_gpu(self, x, idx_unshuffle):\n",
    "        \"\"\"\n",
    "        Undo batch shuffle.\n",
    "        \"\"\"\n",
    "        return x[idx_unshuffle]\n",
    "\n",
    "\n",
    "    def initiate_memorybank(self, dataloader):\n",
    "        print('Initiate memory bank!')\n",
    "        num = 0\n",
    "        iter_data = iter(dataloader)\n",
    "        for i in range(self.K):  # update the memory bank with image representation\n",
    "            if num == self.K:\n",
    "                break\n",
    "            # print(num)\n",
    "            try:\n",
    "                [im_k, _], coarse_label, _ = next(iter_data)\n",
    "            except:\n",
    "                iter_data = iter(dataloader)\n",
    "                [im_k, _], coarse_label, _ = next(iter_data)\n",
    "            num = num + len(im_k)\n",
    "            im_k, coarse_label = im_k.cuda(non_blocking=True), coarse_label.cuda(non_blocking=True)\n",
    "            im_k_, idx_unshufflek = self._batch_shuffle_single_gpu(im_k)\n",
    "            _, k = self.encoder_k(im_k_)  # keys: NxC\n",
    "            k = nn.functional.normalize(k, dim=1)  # already normalized\n",
    "            # undo shuffle\n",
    "            k = self._batch_unshuffle_single_gpu(k, idx_unshufflek)\n",
    "            self._dequeue_and_enqueue(k, coarse_label)\n",
    "\n",
    "    def forward(self, im_k, im_q, coarse_label, args):\n",
    "        with torch.no_grad():  # no gradient to keys\n",
    "            self._momentum_update_key_encoder()\n",
    "        cls_q, q = self.encoder_q(im_q)  # queries:\n",
    "        q = nn.functional.normalize(q, dim=1)  # already normalized\n",
    "\n",
    "        # compute key features\n",
    "        with torch.no_grad():  # no gradient to keys\n",
    "            # shuffle for making use of BN\n",
    "            im_k_, idx_unshufflek = self._batch_shuffle_single_gpu(im_k)\n",
    "            _, k = self.encoder_k(im_k_)  # keys: NxC\n",
    "            k = nn.functional.normalize(k, dim=1)  # already normalized\n",
    "            # undo shuffle\n",
    "            k = self._batch_unshuffle_single_gpu(k, idx_unshufflek)\n",
    "            \n",
    "            \"\"\"\n",
    "            # soft-labels\n",
    "            coarse_z = torch.ones(len(q), self.K).cuda()\n",
    "            new_label = coarse_label.reshape(-1, 1).repeat(1, self.K)\n",
    "            memory_labels = self.coarse_labels.reshape(1, -1).repeat(len(q), 1)\n",
    "            coarse_z = coarse_z * (new_label == memory_labels)\n",
    "            logits_pd = torch.einsum('nc,ck->nk', [k, self.queue.clone().detach()])\n",
    "            logits_pd /= self.T2\n",
    "            logits_pd = logits_pd * coarse_z  # mask out non-same-coarse class samples\n",
    "            logits_pd = logits_pd - logits_pd.max(dim=1, keepdim=True)[0]\n",
    "            pseudo_soft_z = logits_pd.exp() * coarse_z\n",
    "            pseudo_sum = torch.sum(pseudo_soft_z, dim=1, keepdim=True)\n",
    "            maskcon_z = torch.zeros(len(q), self.K + 1).cuda()\n",
    "            maskcon_z[:, 0] = 1\n",
    "            tmp = pseudo_soft_z / pseudo_sum\n",
    "            # rescale by maximum\n",
    "            tmp = tmp / tmp.max(dim=1, keepdim=True)[0]\n",
    "            maskcon_z[:, 1:] = tmp\n",
    "            # generate weighted inter-sample relations\n",
    "            maskcon_z = maskcon_z / maskcon_z.sum(dim=1, keepdim=True)\n",
    "\n",
    "            # self-supervised inter-sample relations\n",
    "            self_z = torch.zeros(len(q), self.K + 1).cuda()\n",
    "            self_z[:, 0] = 1.0\n",
    "\n",
    "            labels = args.w * maskcon_z + (1 - args.w) * self_z\n",
    "            \"\"\"\n",
    "\n",
    "        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])\n",
    "        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n",
    "        \n",
    "        \n",
    "        align_loss = 2- 2 * l_pos.mean()\n",
    "        \n",
    "        sq_dists = (2 - 2 * l_neg)\n",
    "        \n",
    "        mask = (coarse_label.view(-1,1) == self.coarse_labels.view(1,-1))\n",
    "        \n",
    "        all_sq_dists = torch.cat([sq_dists, torch.norm(q[:, None] - q, dim=2, p=2).pow(2)], 1)\n",
    "        all_coarse_labels = torch.cat([self.coarse_labels, coarse_label])\n",
    "        mask = (coarse_label.view(-1,1) == all_coarse_labels.view(1,-1))\n",
    "        sqdists_2_average = all_sq_dists[mask].flatten()\n",
    "        loss_unif = sqdists_2_average.mul(-1/self.T1).exp().mean().log()\n",
    "        \n",
    "        \n",
    "        loss = (align_loss / self.T1) + loss_unif \n",
    "        \n",
    "        \n",
    "        self._dequeue_and_enqueue(k, coarse_label)\n",
    "        \n",
    "        \"\"\"\n",
    "            l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])\n",
    "            l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n",
    "\n",
    "            align_loss = 2- 2 * l_pos.mean()\n",
    "\n",
    "            sq_dists = (2 - 2 * l_pos).flatten()\n",
    "            sq_dists = torch.cat([sq_dists, torch.pdist(q, p=2).pow(2)])\n",
    "\n",
    "            loss_unif = sq_dists.mul(-1/self.T1).exp().mean().log()\n",
    "\n",
    "\n",
    "            loss = (align_loss / self.T1) + loss_unif \n",
    "\n",
    "            # inside vs outside?\n",
    "            self._dequeue_and_enqueue(k, coarse_label)\n",
    "        \"\"\"\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2783f236-e616-47ab-bb5d-4c205cef112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"### Set arguments\"\"\"\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Masked contrastive learning.')\n",
    "\n",
    "# training config:\n",
    "parser.add_argument('--dataset', default='cifar100', choices=['cifar100', 'cifartoy_bad', 'cifartoy_good', 'cars196', 'sop_split1', 'sop_split2', 'imagenet32'], type=str, help='train dataset')\n",
    "parser.add_argument('--data_path', default='../datasets/cifar100', type=str, help='train dataset')\n",
    "\n",
    "# model configs: [Almost fixed for all experiments]\n",
    "parser.add_argument('--arch', default='resnet18')\n",
    "parser.add_argument('--dim', default=256, type=int, help='feature dimension')\n",
    "parser.add_argument('--K', default=8192, type=int, help='queue size; number of negative keys')\n",
    "parser.add_argument('--m', default=0.99, type=float, help='moco momentum of updating key encoder')\n",
    "parser.add_argument('--t0', default=0.1, type=float, help='softmax temperature for training')\n",
    "\n",
    "# train configs:\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.02, type=float, metavar='LR', help='initial learning rate', dest='lr')\n",
    "parser.add_argument('--epochs', default=200, type=int, metavar='N', help='number of total epochs')\n",
    "parser.add_argument('--warm_up', default=5, type=int, metavar='N', help='number of warmup epochs')\n",
    "parser.add_argument('--batch_size', default=128, type=int, metavar='N', help='mini-batch size')\n",
    "parser.add_argument('--wd', default=5e-4, type=float, metavar='W', help='weight decay')\n",
    "parser.add_argument('--aug_q', default='strong', type=str, help='augmentation strategy for query image')\n",
    "parser.add_argument('--aug_k', default='weak', type=str, help='augmentation strategy for key image')\n",
    "parser.add_argument('--gpu_id', default='0', type=str, help='gpuid')\n",
    "\n",
    "# method configs:\n",
    "parser.add_argument('--mode', default='maskcon', type=str, choices=['maskcon', 'grafit', 'coins'], help='training mode')\n",
    "\n",
    "# maskcon-specific hyperparameters:\n",
    "parser.add_argument('--w', default=0.5, type=float, help='weight of self-invariance')  # not-used if maskcon\n",
    "parser.add_argument('--t', default=0.05, type=float, help='softmax temperature weight for soft label')\n",
    "\n",
    "# logger configs\n",
    "parser.add_argument('--wandb_id', type=str, default=\"cifar100\",help='wandb user id')\n",
    "\n",
    "\n",
    "# train for one epoch\n",
    "def train(net, data_loader, train_optimizer, epoch, args):\n",
    "    net.train()\n",
    "    losses, total_num = 0.0, 0.0\n",
    "    train_bar = tqdm(data_loader)\n",
    "    for i, [[im_k, im_q], coarse_targets, fine_targets] in enumerate(train_bar):\n",
    "        adjust_learning_rate(train_optimizer, args.warm_up, epoch, args.epochs, args.lr, i, data_loader.__len__())\n",
    "        im_k, im_q, coarse_targets, fine_targets = im_k.cuda(), im_q.cuda(), coarse_targets.cuda(), fine_targets.cuda()\n",
    "        if args.mode == 'grafit' or args.mode == 'coins':\n",
    "            loss = net.forward_explicit(im_k, im_q, coarse_targets, args)\n",
    "        else:  # if args.mode == 'maskcon'\n",
    "            loss = net(im_k, im_q, coarse_targets, args)\n",
    "        train_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_optimizer.step()\n",
    "\n",
    "        total_num += im_k.shape[0]\n",
    "        losses += loss.item() * im_k.shape[0]\n",
    "        train_bar.set_description(\n",
    "            'Train Epoch: [{}/{}], lr: {:.6f}, Loss: {:.4f}'.format(\n",
    "                epoch, args.epochs,\n",
    "                train_optimizer.param_groups[0]['lr'],\n",
    "                losses / total_num\n",
    "            ))\n",
    "\n",
    "    return losses / total_num\n",
    "\n",
    "\n",
    "def retrieval(encoder, test_loader, K, chunks=10):\n",
    "    encoder.eval()\n",
    "    feature_bank, target_bank = [], []\n",
    "    with torch.no_grad():\n",
    "        # for i, (image, _, fine_label) in enumerate(tqdm(test_loader, desc='Retrieval ...')):\n",
    "        for i, (image, _, fine_label) in enumerate(test_loader):\n",
    "            image = image.cuda(non_blocking=True)\n",
    "            label = fine_label.cuda(non_blocking=True)\n",
    "            output = encoder(image, feat=True)\n",
    "            feature_bank.append(output)\n",
    "            target_bank.append(label)\n",
    "\n",
    "        feature = F.normalize(torch.cat(feature_bank, dim=0), dim=1)\n",
    "        label = torch.cat(target_bank, dim=0).contiguous()\n",
    "    label = label.unsqueeze(-1)\n",
    "    feat_norm = F.normalize(feature, dim=1)\n",
    "    split = torch.tensor(np.linspace(0, len(feat_norm), chunks + 1, dtype=int), dtype=torch.long).to(feature.device)\n",
    "    recall = [[] for i in K]\n",
    "    ids = [torch.tensor([]).to(feature.device) for i in K]\n",
    "    correct = [torch.tensor([]).to(feature.device) for i in K]\n",
    "    k_max = np.max(K)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j in range(chunks):\n",
    "            torch.cuda.empty_cache()\n",
    "            part_feature = feat_norm[split[j]: split[j + 1]]\n",
    "            similarity = torch.einsum('ab,bc->ac', part_feature, feat_norm.T)\n",
    "\n",
    "            topmax = similarity.topk(k_max + 1)[1][:, 1:]\n",
    "            del similarity\n",
    "            retrievalmax = label[topmax].squeeze()\n",
    "            for k, i in enumerate(K):\n",
    "                anchor_label = label[split[j]: split[j + 1]].repeat(1, i)\n",
    "                topi = topmax[:, :i]\n",
    "                retrieval_label = retrievalmax[:, :i]\n",
    "                correct_i = torch.sum(anchor_label == retrieval_label, dim=1, keepdim=True)\n",
    "                correct[k] = torch.cat([correct[k], correct_i], dim=0)\n",
    "                ids[k] = torch.cat([ids[k], topi], dim=0)\n",
    "\n",
    "        # calculate recall @ K\n",
    "        num_sample = len(feat_norm)\n",
    "        for k, i in enumerate(K):\n",
    "            acc_k = float((correct[k] > 0).int().sum() / num_sample)\n",
    "            recall[k] = acc_k\n",
    "\n",
    "        ##################################################################\n",
    "        # calculate precision @ K\n",
    "        # precision = [[] for i in K]\n",
    "        # num_sample = len(feat_norm)\n",
    "        # for k, i in enumerate(K):\n",
    "        #     acc_k = float((correct[k]).int().sum() / num_sample)\n",
    "        #     precision[k] = acc_k / i\n",
    "        ##################################################################\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "def main_proc(args, model, train_loader, test_loader):\n",
    "    # wandb.init(project=args.mode, entity=args.wandb_id, name='train_' + args.results_dir, group=f'train_{args.dataset}_{args.mode}')\n",
    "    # wandb.config.update(args)\n",
    "    \"\"\"### Start training\"\"\"\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.wd, momentum=0.9)\n",
    "    epoch_start = 0\n",
    "\n",
    "    with open(f'{args.wandb_id}/{args.results_dir}' + '/args.json', 'w') as fid:\n",
    "        json.dump(args.__dict__, fid, indent=2)\n",
    "\n",
    "    train_logs = open(f'{args.wandb_id}/{args.results_dir}/train_logs.txt', 'w')\n",
    "\n",
    "    # training loop\n",
    "    best_retrieval_top1 = 0\n",
    "    best_retrieval_top2 = 0\n",
    "    best_retrieval_top5 = 0\n",
    "    best_retrieval_top10 = 0\n",
    "    best_retrieval_top50 = 0\n",
    "    best_retrieval_top100 = 0\n",
    "\n",
    "    model.initiate_memorybank(train_loader)\n",
    "\n",
    "    for epoch in range(epoch_start, args.epochs):\n",
    "        if epoch % 10 == 0:\n",
    "            retrieval_topk = retrieval(model.encoder_q, test_loader, [1, 2, 5, 10, 50, 100])\n",
    "            retrieval_top1, retrieval_top2, retrieval_top5, retrieval_top10, retrieval_top50, retrieval_top100 = retrieval_topk\n",
    "            if retrieval_top1 > best_retrieval_top1:\n",
    "                best_retrieval_top1 = best_retrieval_top1\n",
    "            if retrieval_top2 > best_retrieval_top2:\n",
    "                best_retrieval_top2 = best_retrieval_top2\n",
    "            if retrieval_top5 > best_retrieval_top5:\n",
    "                best_retrieval_top5 = best_retrieval_top5\n",
    "            if retrieval_top10 > best_retrieval_top10:\n",
    "                best_retrieval_top10 = best_retrieval_top10\n",
    "            if retrieval_top50 > best_retrieval_top50:\n",
    "                best_retrieval_top50 = best_retrieval_top50\n",
    "            if retrieval_top100 > best_retrieval_top100:\n",
    "                best_retrieval_top100 = best_retrieval_top100\n",
    "\n",
    "            # wandb.log({'R@1': retrieval_top1, 'R@2': retrieval_top2, 'R@5': retrieval_top5, 'R@10': retrieval_top10, 'R@50': retrieval_top50, 'R@100': retrieval_top100}, step=epoch)\n",
    "            # save statistics\n",
    "            print(f'Epoch [{epoch}/{args.epochs}]: R@1: {retrieval_top1:.4f}, R@2: {retrieval_top2:.4f}, R@5: {retrieval_top5:.4f}, R@10: {retrieval_top10:.4f},  R@50: {retrieval_top50:.4f},R@100: {retrieval_top100:.4f}')\n",
    "            train_logs.write(\n",
    "                f'Epoch [{epoch}/{args.epochs}]: R@1: {retrieval_top1:.4f}, R@2: {retrieval_top2:.4f}, R@5: {retrieval_top5:.4f}, R@10: {retrieval_top10:.4f},  R@50: {retrieval_top50:.4f},R@100: {retrieval_top100:.4f}\\n')\n",
    "            train_logs.flush()\n",
    "\n",
    "        train(model, train_loader, optimizer, epoch, args)\n",
    "    # wandb.finish()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb2758b-dea2-4386-bf27-10431f264c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(K=8192, arch='resnet18', aug_k='weak', aug_q='strong', batch_size=128, data_path='../datasets/cifar100', dataset='cifar100', dim=256, epochs=200, gpu_id='0', lr=0.02, m=0.99, mode='maskcon', t=0.05, t0=0.1, w=0.5, wandb_id='cifar100', warm_up=5, wd=0.0005)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Initiate memory bank!\n",
      "Epoch [0/200]: R@1: 0.1181, R@2: 0.1708, R@5: 0.2727, R@10: 0.3843,  R@50: 0.7080,R@100: 0.8366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [0/200], lr: 0.003990, Loss: -0.9655: 100%|██████████| 390/390 [00:20<00:00, 19.22it/s]\n",
      "Train Epoch: [1/200], lr: 0.007990, Loss: -1.8940: 100%|██████████| 390/390 [00:19<00:00, 19.56it/s]\n",
      "Train Epoch: [2/200], lr: 0.011990, Loss: -2.3971: 100%|██████████| 390/390 [00:20<00:00, 19.46it/s]\n",
      "Train Epoch: [3/200], lr: 0.015990, Loss: -2.6796: 100%|██████████| 390/390 [00:20<00:00, 19.48it/s]\n",
      "Train Epoch: [4/200], lr: 0.019990, Loss: -2.8636: 100%|██████████| 390/390 [00:20<00:00, 19.44it/s]\n",
      "Train Epoch: [5/200], lr: 0.019999, Loss: -2.9757: 100%|██████████| 390/390 [00:20<00:00, 19.48it/s]\n",
      "Train Epoch: [6/200], lr: 0.019995, Loss: -3.0989: 100%|██████████| 390/390 [00:20<00:00, 19.47it/s]\n",
      "Train Epoch: [7/200], lr: 0.019988, Loss: -3.1727: 100%|██████████| 390/390 [00:20<00:00, 19.21it/s]\n",
      "Train Epoch: [8/200], lr: 0.019979, Loss: -3.2075: 100%|██████████| 390/390 [00:20<00:00, 19.43it/s]\n",
      "Train Epoch: [9/200], lr: 0.019968, Loss: -3.2483: 100%|██████████| 390/390 [00:20<00:00, 19.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200]: R@1: 0.2527, R@2: 0.3494, R@5: 0.5026, R@10: 0.6205,  R@50: 0.8753,R@100: 0.9401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [10/200], lr: 0.019953, Loss: -3.2574: 100%|██████████| 390/390 [00:20<00:00, 19.43it/s]\n",
      "Train Epoch: [11/200], lr: 0.019937, Loss: -3.2810: 100%|██████████| 390/390 [00:20<00:00, 19.42it/s]\n",
      "Train Epoch: [12/200], lr: 0.019917, Loss: -3.3255: 100%|██████████| 390/390 [00:20<00:00, 19.40it/s]\n",
      "Train Epoch: [13/200], lr: 0.019895, Loss: -3.3201: 100%|██████████| 390/390 [00:20<00:00, 19.44it/s]\n",
      "Train Epoch: [14/200], lr: 0.019871, Loss: -3.3377: 100%|██████████| 390/390 [00:20<00:00, 19.38it/s]\n",
      "Train Epoch: [15/200], lr: 0.019843, Loss: -3.3612: 100%|██████████| 390/390 [00:19<00:00, 19.50it/s]\n",
      "Train Epoch: [16/200], lr: 0.019814, Loss: -3.3576: 100%|██████████| 390/390 [00:20<00:00, 19.44it/s]\n",
      "Train Epoch: [17/200], lr: 0.019782, Loss: -3.3747: 100%|██████████| 390/390 [00:20<00:00, 19.41it/s]\n",
      "Train Epoch: [18/200], lr: 0.019747, Loss: -3.3554: 100%|██████████| 390/390 [00:20<00:00, 19.40it/s]\n",
      "Train Epoch: [19/200], lr: 0.019710, Loss: -3.3786: 100%|██████████| 390/390 [00:20<00:00, 19.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200]: R@1: 0.2408, R@2: 0.3352, R@5: 0.4870, R@10: 0.6084,  R@50: 0.8625,R@100: 0.9360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [20/200], lr: 0.019670, Loss: -3.3836: 100%|██████████| 390/390 [00:20<00:00, 19.39it/s]\n",
      "Train Epoch: [21/200], lr: 0.019627, Loss: -3.4211: 100%|██████████| 390/390 [00:20<00:00, 19.37it/s]\n",
      "Train Epoch: [22/200], lr: 0.019583, Loss: -3.4113: 100%|██████████| 390/390 [00:20<00:00, 19.42it/s]\n",
      "Train Epoch: [23/200], lr: 0.019535, Loss: -3.4285: 100%|██████████| 390/390 [00:20<00:00, 19.39it/s]\n",
      "Train Epoch: [24/200], lr: 0.019485, Loss: -3.4307: 100%|██████████| 390/390 [00:20<00:00, 19.42it/s]\n",
      "Train Epoch: [25/200], lr: 0.019433, Loss: -3.4467: 100%|██████████| 390/390 [00:20<00:00, 19.41it/s]\n",
      "Train Epoch: [26/200], lr: 0.019379, Loss: -3.4540: 100%|██████████| 390/390 [00:19<00:00, 19.54it/s]\n",
      "Train Epoch: [27/200], lr: 0.019321, Loss: -3.4640: 100%|██████████| 390/390 [00:20<00:00, 19.47it/s]\n",
      "Train Epoch: [28/200], lr: 0.019262, Loss: -3.4716: 100%|██████████| 390/390 [00:20<00:00, 19.36it/s]\n",
      "Train Epoch: [29/200], lr: 0.019200, Loss: -3.4648: 100%|██████████| 390/390 [00:19<00:00, 19.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/200]: R@1: 0.2384, R@2: 0.3382, R@5: 0.4862, R@10: 0.6142,  R@50: 0.8646,R@100: 0.9321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [30/200], lr: 0.019136, Loss: -3.4998: 100%|██████████| 390/390 [00:20<00:00, 19.43it/s]\n",
      "Train Epoch: [31/200], lr: 0.019069, Loss: -3.4876: 100%|██████████| 390/390 [00:20<00:00, 19.45it/s]\n",
      "Train Epoch: [32/200], lr: 0.019000, Loss: -3.5036: 100%|██████████| 390/390 [00:20<00:00, 19.42it/s]\n",
      "Train Epoch: [33/200], lr: 0.018928, Loss: -3.5090: 100%|██████████| 390/390 [00:20<00:00, 19.40it/s]\n",
      "Train Epoch: [34/200], lr: 0.018855, Loss: -3.5284: 100%|██████████| 390/390 [00:20<00:00, 19.42it/s]\n",
      "Train Epoch: [35/200], lr: 0.018779, Loss: -3.5285: 100%|██████████| 390/390 [00:20<00:00, 19.45it/s]\n",
      "Train Epoch: [36/200], lr: 0.018700, Loss: -3.5309: 100%|██████████| 390/390 [00:20<00:00, 19.45it/s]\n",
      "Train Epoch: [37/200], lr: 0.018620, Loss: -3.5233: 100%|██████████| 390/390 [00:20<00:00, 19.44it/s]\n",
      "Train Epoch: [38/200], lr: 0.018537, Loss: -3.5567: 100%|██████████| 390/390 [00:20<00:00, 19.38it/s]\n",
      "Train Epoch: [39/200], lr: 0.018452, Loss: -3.5610: 100%|██████████| 390/390 [00:20<00:00, 19.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/200]: R@1: 0.2443, R@2: 0.3418, R@5: 0.4893, R@10: 0.6086,  R@50: 0.8629,R@100: 0.9334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [40/200], lr: 0.018365, Loss: -3.5718: 100%|██████████| 390/390 [00:19<00:00, 19.50it/s]\n",
      "Train Epoch: [41/200], lr: 0.018276, Loss: -3.5860: 100%|██████████| 390/390 [00:20<00:00, 19.24it/s]\n",
      "Train Epoch: [42/200], lr: 0.018184, Loss: -3.5761: 100%|██████████| 390/390 [00:20<00:00, 19.41it/s]\n",
      "Train Epoch: [43/200], lr: 0.018090, Loss: -3.5704: 100%|██████████| 390/390 [00:20<00:00, 19.46it/s]\n",
      "Train Epoch: [44/200], lr: 0.017995, Loss: -3.5808: 100%|██████████| 390/390 [00:20<00:00, 19.49it/s]\n",
      "Train Epoch: [45/200], lr: 0.017897, Loss: -3.5655: 100%|██████████| 390/390 [00:20<00:00, 19.44it/s]\n",
      "Train Epoch: [46/200], lr: 0.017797, Loss: -3.5760: 100%|██████████| 390/390 [00:20<00:00, 19.44it/s]\n",
      "Train Epoch: [47/200], lr: 0.017695, Loss: -3.5835: 100%|██████████| 390/390 [00:20<00:00, 19.33it/s]\n",
      "Train Epoch: [48/200], lr: 0.017591, Loss: -3.6040: 100%|██████████| 390/390 [00:20<00:00, 19.41it/s]\n",
      "Train Epoch: [49/200], lr: 0.017485, Loss: -3.5785: 100%|██████████| 390/390 [00:20<00:00, 19.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/200]: R@1: 0.2427, R@2: 0.3360, R@5: 0.4896, R@10: 0.6124,  R@50: 0.8624,R@100: 0.9327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [50/200], lr: 0.017378, Loss: -3.6074: 100%|██████████| 390/390 [00:20<00:00, 19.40it/s]\n",
      "Train Epoch: [51/200], lr: 0.017268, Loss: -3.6209: 100%|██████████| 390/390 [00:20<00:00, 19.42it/s]\n",
      "Train Epoch: [52/200], lr: 0.017156, Loss: -3.6181: 100%|██████████| 390/390 [00:20<00:00, 19.42it/s]\n",
      "Train Epoch: [53/200], lr: 0.017043, Loss: -3.6181: 100%|██████████| 390/390 [00:20<00:00, 19.39it/s]\n",
      "Train Epoch: [54/200], lr: 0.016928, Loss: -3.6355: 100%|██████████| 390/390 [00:20<00:00, 19.40it/s]\n",
      "Train Epoch: [55/200], lr: 0.016810, Loss: -3.6203: 100%|██████████| 390/390 [00:20<00:00, 19.39it/s]\n",
      "Train Epoch: [56/200], lr: 0.016692, Loss: -3.6080: 100%|██████████| 390/390 [00:20<00:00, 19.37it/s]\n",
      "Train Epoch: [57/200], lr: 0.016571, Loss: -3.6575: 100%|██████████| 390/390 [00:19<00:00, 19.50it/s]\n",
      "Train Epoch: [58/200], lr: 0.016449, Loss: -3.6249: 100%|██████████| 390/390 [00:20<00:00, 19.50it/s]\n",
      "Train Epoch: [59/200], lr: 0.016325, Loss: -3.6396: 100%|██████████| 390/390 [00:20<00:00, 19.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/200]: R@1: 0.2543, R@2: 0.3538, R@5: 0.5018, R@10: 0.6177,  R@50: 0.8635,R@100: 0.9352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [60/200], lr: 0.016199, Loss: -3.6360: 100%|██████████| 390/390 [00:20<00:00, 19.42it/s]\n",
      "Train Epoch: [61/200], lr: 0.016072, Loss: -3.6446: 100%|██████████| 390/390 [00:20<00:00, 19.42it/s]\n",
      "Train Epoch: [62/200], lr: 0.015943, Loss: -3.6594: 100%|██████████| 390/390 [00:20<00:00, 19.43it/s]\n",
      "Train Epoch: [63/200], lr: 0.015813, Loss: -3.6563: 100%|██████████| 390/390 [00:20<00:00, 19.39it/s]\n",
      "Train Epoch: [64/200], lr: 0.015681, Loss: -3.6588: 100%|██████████| 390/390 [00:20<00:00, 19.44it/s]\n",
      "Train Epoch: [65/200], lr: 0.015548, Loss: -3.6724: 100%|██████████| 390/390 [00:20<00:00, 19.42it/s]\n",
      "Train Epoch: [66/200], lr: 0.015413, Loss: -3.6591: 100%|██████████| 390/390 [00:20<00:00, 19.41it/s]\n",
      "Train Epoch: [67/200], lr: 0.015277, Loss: -3.6680: 100%|██████████| 390/390 [00:20<00:00, 19.38it/s]\n",
      "Train Epoch: [68/200], lr: 0.015139, Loss: -3.6799: 100%|██████████| 390/390 [00:20<00:00, 19.44it/s]\n",
      "Train Epoch: [69/200], lr: 0.015000, Loss: -3.6851: 100%|██████████| 390/390 [00:20<00:00, 19.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/200]: R@1: 0.2660, R@2: 0.3599, R@5: 0.5038, R@10: 0.6276,  R@50: 0.8722,R@100: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [70/200], lr: 0.014860, Loss: -3.6696: 100%|██████████| 390/390 [00:20<00:00, 19.43it/s]\n",
      "Train Epoch: [71/200], lr: 0.014719, Loss: -3.6870: 100%|██████████| 390/390 [00:20<00:00, 19.47it/s]\n",
      "Train Epoch: [72/200], lr: 0.014576, Loss: -3.6738: 100%|██████████| 390/390 [00:20<00:00, 19.47it/s]\n",
      "Train Epoch: [73/200], lr: 0.014432, Loss: -3.6940: 100%|██████████| 390/390 [00:19<00:00, 19.55it/s]\n",
      "Train Epoch: [74/200], lr: 0.014287, Loss: -3.6974: 100%|██████████| 390/390 [00:20<00:00, 19.41it/s]\n",
      "Train Epoch: [75/200], lr: 0.014141, Loss: -3.6916: 100%|██████████| 390/390 [00:20<00:00, 19.41it/s]\n",
      "Train Epoch: [76/200], lr: 0.013994, Loss: -3.6812: 100%|██████████| 390/390 [00:20<00:00, 19.39it/s]\n",
      "Train Epoch: [77/200], lr: 0.013846, Loss: -3.6895: 100%|██████████| 390/390 [00:19<00:00, 19.50it/s]\n",
      "Train Epoch: [78/200], lr: 0.013697, Loss: -3.6856: 100%|██████████| 390/390 [00:20<00:00, 19.49it/s]\n",
      "Train Epoch: [79/200], lr: 0.013546, Loss: -3.6856: 100%|██████████| 390/390 [00:20<00:00, 19.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/200]: R@1: 0.2641, R@2: 0.3629, R@5: 0.5126, R@10: 0.6303,  R@50: 0.8692,R@100: 0.9383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [80/200], lr: 0.013395, Loss: -3.6900: 100%|██████████| 390/390 [00:19<00:00, 19.55it/s]\n",
      "Train Epoch: [81/200], lr: 0.013243, Loss: -3.7098: 100%|██████████| 390/390 [00:20<00:00, 19.47it/s]\n",
      "Train Epoch: [82/200], lr: 0.013091, Loss: -3.7129: 100%|██████████| 390/390 [00:20<00:00, 19.46it/s]\n",
      "Train Epoch: [83/200], lr: 0.012937, Loss: -3.7079: 100%|██████████| 390/390 [00:20<00:00, 19.37it/s]\n",
      "Train Epoch: [84/200], lr: 0.012783, Loss: -3.7145: 100%|██████████| 390/390 [00:20<00:00, 19.40it/s]\n",
      "Train Epoch: [85/200], lr: 0.012627, Loss: -3.7196: 100%|██████████| 390/390 [00:20<00:00, 19.42it/s]\n",
      "Train Epoch: [86/200], lr: 0.012472, Loss: -3.6897: 100%|██████████| 390/390 [00:20<00:00, 19.42it/s]\n",
      "Train Epoch: [87/200], lr: 0.012315, Loss: -3.7205: 100%|██████████| 390/390 [00:20<00:00, 19.44it/s]\n",
      "Train Epoch: [88/200], lr: 0.012158, Loss: -3.7376: 100%|██████████| 390/390 [00:20<00:00, 19.39it/s]\n",
      "Train Epoch: [89/200], lr: 0.012063, Loss: -3.7322:  61%|██████    | 236/390 [00:12<00:07, 19.81it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "args = parser.parse_args(\"\")\n",
    "print(args)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_id\n",
    "random.seed(1228)\n",
    "torch.manual_seed(1228)\n",
    "torch.cuda.manual_seed_all(1228)\n",
    "np.random.seed(1228)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\"\"\"Define train/test\"\"\"\n",
    "query_transform = get_augment(args.dataset, args.aug_q)\n",
    "key_transform = get_augment(args.dataset, args.aug_k)\n",
    "test_transform = get_augment(args.dataset)\n",
    "\n",
    "if args.dataset == 'cars196':\n",
    "    train_dataset = CARS196(root=args.data_path, split='train', transform=DMixTransform([key_transform, query_transform], [1, 1]))\n",
    "    test_dataset = CARS196(root=args.data_path, split='test', transform=test_transform)\n",
    "    args.num_classes = 8\n",
    "    args.size = 224\n",
    "\n",
    "elif args.dataset == 'cifar100':\n",
    "    train_dataset = CIFAR100(root=args.data_path, download=True, transform=DMixTransform([key_transform, query_transform], [1, 1]))\n",
    "    test_dataset = CIFAR100(root=args.data_path, train=False, download=True, transform=test_transform)\n",
    "    args.num_classes = 20\n",
    "    args.size = 32\n",
    "\n",
    "elif args.dataset == 'cifartoy_good':\n",
    "    train_dataset = CIFARtoy(root=args.data_path, split='good', download=True, transform=DMixTransform([key_transform, query_transform], [1, 1]))\n",
    "    test_dataset = CIFARtoy(root=args.data_path, split='good', train=False, download=True, transform=test_transform)\n",
    "    args.num_classes = 2\n",
    "    args.size = 32\n",
    "\n",
    "elif args.dataset == 'cifartoy_bad':\n",
    "    train_dataset = CIFARtoy(root=args.data_path, split='bad', download=True, transform=DMixTransform([key_transform, query_transform], [1, 1]))\n",
    "    test_dataset = CIFARtoy(root=args.data_path, split='bad', train=False, download=True, transform=test_transform)\n",
    "    args.num_classes = 2\n",
    "    args.size = 32\n",
    "\n",
    "elif args.dataset == 'sop_split2':\n",
    "    train_dataset = StanfordOnlineProducts(split='2', root=args.data_path, train=True, transform=DMixTransform([key_transform, query_transform], [1, 1]))\n",
    "    test_dataset = StanfordOnlineProducts(split='2', root=args.data_path, train=False, transform=test_transform)\n",
    "    args.num_classes = 12\n",
    "    args.size = 224\n",
    "\n",
    "elif args.dataset == 'sop_split1':\n",
    "    train_dataset = StanfordOnlineProducts(split='1', root=args.data_path, train=True, transform=DMixTransform([key_transform, query_transform], [1, 1]))\n",
    "    test_dataset = StanfordOnlineProducts(split='1', root=args.data_path, train=False, transform=test_transform)\n",
    "    args.num_classes = 12\n",
    "    args.size = 224\n",
    "\n",
    "elif args.dataset == 'imagenet32':\n",
    "    train_dataset = ImageNetDownSample(root=args.data_path, train=True, transform=DMixTransform([key_transform, query_transform], [1, 1]))\n",
    "    test_dataset = ImageNetDownSample(root=args.data_path, train=False, transform=test_transform)\n",
    "    args.num_classes = 12\n",
    "    args.size = 32\n",
    "\n",
    "else:\n",
    "    raise ValueError(f'{args.dataset} is not supported now!')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# create trainer\n",
    "trainer = MaskConNew(num_classes_coarse=args.num_classes, dim=args.dim, K=args.K, m=args.m, T1=args.t0, arch=args.arch, size=args.size, T2=args.t, mode=args.mode).cuda()\n",
    "\n",
    "args.results_dir = f'arch_[{args.arch}]_data[{args.dataset}]_epochs[{args.epochs}]_memorysize[{args.K}]_mode[{args.mode}]_contrastive_temperature[{args.t0}]_temperature_maskcon[{args.t}]_weight[{args.w}]]'\n",
    "\n",
    "if not os.path.exists(args.wandb_id):\n",
    "    os.mkdir(args.wandb_id)\n",
    "if not os.path.exists(f'{args.wandb_id}/{args.results_dir}'):\n",
    "    os.mkdir(f'{args.wandb_id}/{args.results_dir}')\n",
    "\n",
    "main_proc(args, trainer, train_loader, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7950c7ec-9808-4850-8b46-ea7eb7cc80f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.wandb_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f1a6c-a3c0-4f2b-a7f5-eccd70647949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
